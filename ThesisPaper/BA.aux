\relax 
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand \oddpage@label [2]{}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{BA.ist}
\@glsorder{word}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{attention}
\citation{bert}
\citation{study}
\citation{webster}
\citation{facebook}
\citation{socialbots}
\citation{notsure}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{hyperpartisannewsdetection}
\citation{neural}
\citation{wb1}
\citation{s2s}
\citation{pre1}
\citation{pre2}
\citation{pre3}
\citation{bert}
\citation{neural}
\citation{infor}
\citation{elmo}
\citation{gewinner}
\citation{vernon}
\citation{glove}
\citation{doc2vec}
\citation{use}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{bertgruppe}
\citation{hyperpartisannewsdetection}
\citation{IR-book}
\citation{IR-book}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Fundamentals}{5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Text Representation}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Term Frequency-Inverse Document Frequency}{5}\protected@file@percent }
\citation{distributionalhypothesis}
\citation{distributionalhypothesis}
\citation{fasttext}
\citation{glove}
\citation{effiecientestimation}
\citation{word2vecparam}
\citation{word2vecparam}
\citation{word2vecparam}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Word Embeddings}{6}\protected@file@percent }
\citation{word2vecparam}
\citation{word2vecparam}
\citation{btheory}
\citation{multinomialnb}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Classification Methods}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Multinomial Naive Bayes Classifier}{7}\protected@file@percent }
\citation{logisticregressionbook}
\citation{algorithms2}
\citation{algorithms2}
\citation{algorithms}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Logistic Regression Classifier}{8}\protected@file@percent }
\citation{liblinear}
\citation{algorithms2}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Decision Trees and Random Forest Classifier}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Decision Trees}{9}\protected@file@percent }
\citation{algorithms}
\citation{algorithms}
\citation{randomforest}
\@writefile{toc}{\contentsline {subsubsection}{Random Forest Classifier}{10}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Random Forest\relax }}{10}\protected@file@percent }
\citation{bert}
\citation{rnn}
\citation{rnn}
\citation{feedforward}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Bidirectional Encoder Representation from Transformers}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Transfer Learning}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \gls {rnn} and Feedforward Neural Network \cite  {rnn}\relax }}{11}\protected@file@percent }
\citation{ELMAN1990179}
\citation{rnnmodel}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{lstm}
\citation{rnn}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Input gate$_1$ \cite  {rnn}\relax }}{13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Input gate$_2$ \cite  {rnn}\relax }}{13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Forget gate \cite  {rnn}\relax }}{13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Output gate \cite  {rnn}\relax }}{13}\protected@file@percent }
\citation{rnn}
\citation{encodedecode}
\citation{encodedecode}
\citation{attention}
\citation{attention}
\citation{bahdnau}
\citation{lunong}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The Transformer - model architecture \cite  {attention}\relax }}{15}\protected@file@percent }
\citation{attention}
\citation{attention}
\citation{residualcon}
\citation{normalization}
\citation{dopout}
\citation{attention}
\citation{attention}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Scaled Dot-Product Attention\relax }}{16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Multi-Head Attention\relax }}{16}\protected@file@percent }
\citation{attention}
\citation{attention}
\citation{attention}
\citation{bert}
\citation{bert}
\citation{bert}
\citation{attention}
\citation{bert}
\citation{bert}
\@writefile{toc}{\contentsline {subsubsection}{\gls {bert} Model}{18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces \gls {bert} Input Representations \cite  {bert}\relax }}{18}\protected@file@percent }
\citation{cloze}
\citation{bert}
\citation{bert}
\citation{bert}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Evaluation}{19}\protected@file@percent }
\citation{confusionmatrix}
\citation{algorithms}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Evaluation Measures}{20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Confusion Matrix\relax }}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Cross Validation}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Grid Search}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Data Description}{22}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Dataset labeled by-Publisher}{23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Hyperpartisan Distribution by-Publisher\relax }}{23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Publishing Years Distribution by-Publisher\relax }}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Dataset labeled by-Article}{24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Hyperpartisan Distribution by-Article\relax }}{24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Publishing Years Distribution by-Article\relax }}{24}\protected@file@percent }
\citation{textclassification}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Classification Techniques}{25}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Data Preparation}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}File Parsing}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Information Filtering}{26}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}Parse Ground-Truth File}{26}\protected@file@percent }
\citation{pandas}
\citation{textclassification}
\citation{nltk}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Combining Data}{27}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.2}Merge GroundTruth and Training datasets}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Special Characters and Stop Word Removal}{27}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.3}Special Characters and Stop Word Removal}{27}\protected@file@percent }
\citation{word2vecdortmund}
\citation{textclassification}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Tokenization and Stemming}{28}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.4}Tokenization and Stemming}{28}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Text Representation}{28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Term Frequency-Inverse Document Frequency}{28}\protected@file@percent }
\citation{scikit-learn}
\citation{gensim}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Word2Vec}{30}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.5}Word2Vec with gensim}{30}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.6}sent\_vectorizer}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Classification Methods}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Classical Approach}{31}\protected@file@percent }
\citation{randomforest}
\citation{tensorflow}
\citation{pytorch}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Novel Approach using Bidirectional Encoder Representations from Transformers}{33}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.7}Create\_examples in BERT}{34}\protected@file@percent }
\citation{adam}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.8}run\_classifier prompt with FLAGS}{35}\protected@file@percent }
\citation{tira}
\citation{evaluationasaservice}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Evaluation}{36}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces TIRA Interface\relax }}{36}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Evaluation Results -- Article: Classifier has been trained on the by-Article dataset; -Publisher: Classifier has been trained on the by-Publisher dataset; -WB: Word Embeddings have been used; -TFIDF: \gls {tfidf}\relax }}{37}\protected@file@percent }
\bibstyle{plain}
\bibdata{BA}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{39}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{tensorflow}{1}
\bibcite{feedforward}{2}
\bibcite{infor}{3}
\bibcite{btheory}{4}
\bibcite{neural}{5}
\bibcite{fasttext}{6}
\bibcite{algorithms}{7}
\bibcite{algorithms2}{8}
\bibcite{randomforest}{9}
\bibcite{use}{10}
\bibcite{encodedecode}{11}
\bibcite{IR-book}{12}
\bibcite{pre1}{13}
\bibcite{bert}{14}
\bibcite{adam}{15}
\bibcite{bertgruppe}{16}
\bibcite{bahdnau}{17}
\bibcite{ELMAN1990179}{18}
\bibcite{liblinear}{19}
\bibcite{study}{20}
\bibcite{rnn}{21}
\bibcite{evaluationasaservice}{22}
\bibcite{logisticregressionbook}{23}
\bibcite{distributionalhypothesis}{24}
\bibcite{residualcon}{25}
\bibcite{lstm}{26}
\bibcite{textclassification}{27}
\bibcite{gewinner}{28}
\bibcite{normalization}{29}
\bibcite{word2vecdortmund}{30}
\bibcite{multinomialnb}{31}
\bibcite{hyperpartisannewsdetection}{32}
\bibcite{socialbots}{33}
\bibcite{doc2vec}{34}
\bibcite{facebook}{35}
\bibcite{lunong}{36}
\bibcite{pre2}{37}
\bibcite{pandas}{38}
\bibcite{webster}{39}
\bibcite{rnnmodel}{40}
\bibcite{pytorch}{41}
\bibcite{scikit-learn}{42}
\bibcite{glove}{43}
\bibcite{elmo}{44}
\bibcite{tira}{45}
\bibcite{gensim}{46}
\bibcite{word2vecparam}{47}
\bibcite{notsure}{48}
\bibcite{dopout}{49}
\bibcite{vernon}{50}
\bibcite{confusionmatrix}{51}
\bibcite{nltk}{52}
\bibcite{pre3}{53}
\bibcite{s2s}{54}
\bibcite{cloze}{55}
\bibcite{wb1}{56}
\bibcite{effiecientestimation}{57}
\bibcite{attention}{58}
