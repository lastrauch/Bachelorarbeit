\relax 
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand \oddpage@label [2]{}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{BA.ist}
\@glsorder{word}
\citation{algorithms}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{attention}
\citation{bert}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Statement}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contribution}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Related Work}{1}\protected@file@percent }
\citation{IR-book}
\citation{IR-book}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Fundamentals}{2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Term Frequency-Inverse Document Frequency}{2}\protected@file@percent }
\citation{distributionalhypothesis}
\citation{effiecientestimation}
\citation{word2vecparam}
\citation{word2vecparam}
\citation{word2vecparam}
\citation{word2vecparam}
\citation{word2vecparam}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Word Embeddings}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Word2Vec}{3}\protected@file@percent }
\citation{btheory}
\citation{multinomialnb}
\citation{logisticregressionbook}
\citation{algorithms2}
\citation{algorithms2}
\citation{algorithms2}
\citation{algorithms}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Multinomial Naive Bayes Classifier}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Logistic Regression Classifier}{4}\protected@file@percent }
\citation{algorithms}
\citation{algorithms}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Sigmoid Function \cite  {algorithms}\relax }}{5}\protected@file@percent }
\citation{liblinear}
\citation{algorithms2}
\citation{algorithms}
\citation{algorithms}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Decision Trees and Random Forest Classifier}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Decision Trees}{6}\protected@file@percent }
\citation{randomforest}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Random Forest Classifier}{7}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Random Forest\relax }}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Cross Validation}{7}\protected@file@percent }
\citation{confusionmatrix}
\citation{algorithms}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Evaluation}{8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Confusion Matrix\relax }}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Data}{9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cha:theory}{{3}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data Description}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Dataset labelled by Publisher}{10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Hyperpartisan Distribution by Publisher\relax }}{10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Publishers with the highest proportion of Hypeprartisan articles\relax }}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Dataset labelled by Article}{10}\protected@file@percent }
\citation{parse}
\citation{iterparse}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Hyperpartisan Distribution by Article\relax }}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Publishing Years Distribution by Article\relax }}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Data Preparation}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}File Parsing}{11}\protected@file@percent }
\citation{pandas}
\citation{textclassification}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Information Filtering}{12}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Parse Ground-Truth File\relax }}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Combine Data}{12}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Merge Groundtruth- and Trainingdatasets\relax }}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Special Characters and Stop Word Removal}{12}\protected@file@percent }
\citation{nltk}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Remove special characters and stop words\relax }}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Tokenization and Stemming of the datasets}{13}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Special Character and Stopword Removal with Pandas and NLTK\relax }}{13}\protected@file@percent }
\citation{textclassification}
\citation{word2vecdortmund}
\citation{textclassification}
\citation{scikit-learn}
\citation{tfidf}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Methodology}{14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Vector Representation of the Text}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Term Frequency-Inverse Document Frequency}{14}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces \gls {tfidf}\relax }}{15}\protected@file@percent }
\citation{gensim}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Top 10 terms by \Gls {tfidf} weight\relax }}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Word Embeddings}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{16}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Word2Vec with gensim\relax }}{16}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Most similar word to 'trump'\relax }}{17}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces sent\_vectorizer\relax }}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Classification Algorithms}{17}\protected@file@percent }
\citation{codemulinomialnb}
\citation{coderandomforest}
\citation{randomforest}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Multinomial Naive Bayes Classifier}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Random Forest Classifier}{18}\protected@file@percent }
\citation{logisticregression}
\citation{codegridsearch}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Logistic Regression Classifier}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Grid Search}{19}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Grid Search Result of the \gls {mnb} Classifier\relax }}{19}\protected@file@percent }
\citation{pickle}
\citation{keyedvectors}
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces Grid Search for \gls {mnb} classifier\relax }}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Training and Classification}{20}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {10}{\ignorespaces Classifier Training\relax }}{20}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {11}{\ignorespaces Make Predictions\relax }}{21}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {12}{\ignorespaces Saving a trained model\relax }}{21}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {13}{\ignorespaces Saving a trained Word2Vec model\relax }}{21}\protected@file@percent }
\citation{bert}
\citation{feedforward}
\citation{rnn}
\citation{rnn}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Bidirectional Encoder Representations from Transformers}{23}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Recurrent Neural Networks}{23}\protected@file@percent }
\citation{ELMAN1990179}
\citation{rnnmodel}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\citation{rnn}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \gls {rnn} and Feedforward Neural Network \cite  {rnn}\relax }}{24}\protected@file@percent }
\citation{lstm}
\citation{rnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Long Short-Term Memory}{25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Input gate$_1$ \cite  {rnn}\relax }}{25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Input gate$_2$ \cite  {rnn}\relax }}{25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Forget gate \cite  {rnn}\relax }}{25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Output gate \cite  {rnn}\relax }}{25}\protected@file@percent }
\citation{rnn}
\citation{encodedecode}
\citation{encodedecode}
\citation{attention}
\citation{attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}\gls {rnn} Encoder-Decoder}{26}\protected@file@percent }
\citation{bahdnau}
\citation{lunong}
\citation{attention}
\citation{residualcon}
\citation{normalization}
\citation{dopout}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Transformer and Attention}{27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The Transformer - model architecture \cite  {attention}\relax }}{27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Scaled Dot-Product Attention\relax }}{28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Multi-Head Attention\relax }}{28}\protected@file@percent }
\citation{bert}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}\gls {bert} Model}{29}\protected@file@percent }
\citation{attention}
\citation{bert}
\citation{bert}
\citation{cloze}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces \gls {bert} Input Representations \cite  {bert}\relax }}{30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Input Representation}{30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Pre-training and Fine-Tuning}{30}\protected@file@percent }
\citation{tensorflow}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Application}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Evaluation}{32}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibdata{BA}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{33}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{locode}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{tensorflow}{1}
\bibcite{feedforward}{2}
\bibcite{btheory}{3}
\bibcite{algorithms}{4}
\bibcite{algorithms2}{5}
\bibcite{randomforest}{6}
\bibcite{encodedecode}{7}
\bibcite{IR-book}{8}
\bibcite{bert}{9}
\bibcite{bahdnau}{10}
\bibcite{parse}{11}
\bibcite{ELMAN1990179}{12}
\bibcite{liblinear}{13}
\bibcite{rnn}{14}
\bibcite{logisticregressionbook}{15}
\bibcite{distributionalhypothesis}{16}
\bibcite{residualcon}{17}
\bibcite{lstm}{18}
\bibcite{textclassification}{19}
\bibcite{normalization}{20}
\bibcite{word2vecdortmund}{21}
\bibcite{multinomialnb}{22}
\bibcite{lunong}{23}
\bibcite{iterparse}{24}
\bibcite{pandas}{25}
\bibcite{rnnmodel}{26}
\bibcite{keyedvectors}{27}
\bibcite{scikit-learn}{28}
\bibcite{pickle}{29}
\bibcite{gensim}{30}
\bibcite{word2vecparam}{31}
\bibcite{coderandomforest}{32}
\bibcite{tfidf}{33}
\bibcite{logisticregression}{34}
\bibcite{codegridsearch}{35}
\bibcite{codemulinomialnb}{36}
\bibcite{dopout}{37}
\bibcite{confusionmatrix}{38}
\bibcite{nltk}{39}
\bibcite{cloze}{40}
\bibcite{effiecientestimation}{41}
\bibcite{attention}{42}
\bibstyle{plain}
