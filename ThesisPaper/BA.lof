\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces \gls {rnn} and Feedforward Neural Network \cite {rnn}\relax }}{11}% 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Input gate$_1$ \cite {rnn}\relax }}{13}% 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Input gate$_2$ \cite {rnn}\relax }}{13}% 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Forget gate \cite {rnn}\relax }}{13}% 
\contentsline {figure}{\numberline {3.5}{\ignorespaces Output gate \cite {rnn}\relax }}{13}% 
\contentsline {figure}{\numberline {3.6}{\ignorespaces The Transformer - model architecture \cite {attention}\relax }}{15}% 
\contentsline {figure}{\numberline {3.7}{\ignorespaces Scaled Dot-Product Attention\relax }}{16}% 
\contentsline {figure}{\numberline {3.8}{\ignorespaces Multi-Head Attention\relax }}{16}% 
\contentsline {figure}{\numberline {3.9}{\ignorespaces \gls {bert} Input Representations \cite {bert}\relax }}{18}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Hyperpartisan Distribution by Publisher\relax }}{23}% 
\contentsline {figure}{\numberline {4.2}{\ignorespaces Publishing Years Distribution by Publisher\relax }}{23}% 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Hyperpartisan Distribution by Article\relax }}{24}% 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Publishing Years Distribution by Article\relax }}{24}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces TIRA Interface\relax }}{37}% 
\addvspace {10\p@ }
